"""
Load testing for performance validation.

Tests system behavior under various load conditions.
"""

import pytest
import asyncio
import time
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
import requests

@pytest.mark.load
class TestPerformance:
    """Performance and load testing."""
    
    def test_health_endpoint_load(self, http_session, test_config, services_ready):
        """Test health endpoint under load."""
        url = f"{test_config['backend_url']}/health"
        
        # Test parameters
        concurrent_requests = 20
        total_requests = 100
        
        def make_request() -> Dict[str, Any]:
            start_time = time.time()
            try:
                response = http_session.get(url, timeout=test_config['timeout'])
                duration = time.time() - start_time
                
                return {
                    'success': response.status_code == 200,
                    'status_code': response.status_code,
                    'duration': duration,
                    'error': None
                }
            except Exception as e:
                duration = time.time() - start_time
                return {
                    'success': False,
                    'status_code': 0,
                    'duration': duration,
                    'error': str(e)
                }
        
        # Execute load test
        results = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
            futures = [executor.submit(make_request) for _ in range(total_requests)]
            
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
        
        total_duration = time.time() - start_time
        
        # Analyze results
        successful_requests = [r for r in results if r['success']]
        failed_requests = [r for r in results if not r['success']]
        
        success_rate = len(successful_requests) / len(results)
        avg_duration = statistics.mean([r['duration'] for r in successful_requests]) if successful_requests else 0
        max_duration = max([r['duration'] for r in successful_requests]) if successful_requests else 0
        min_duration = min([r['duration'] for r in successful_requests]) if successful_requests else 0
        
        requests_per_second = len(results) / total_duration
        
        # Print performance metrics
        print(f\"\\nLoad Test Results (Health Endpoint):\")\n        print(f\"Total requests: {len(results)}\")\n        print(f\"Successful: {len(successful_requests)} ({success_rate:.2%})\")\n        print(f\"Failed: {len(failed_requests)}\")\n        print(f\"Requests/second: {requests_per_second:.2f}\")\n        print(f\"Average response time: {avg_duration:.3f}s\")\n        print(f\"Min response time: {min_duration:.3f}s\")\n        print(f\"Max response time: {max_duration:.3f}s\")\n        \n        # Performance assertions\n        assert success_rate >= 0.95, f\"Success rate too low: {success_rate:.2%}\"\n        assert avg_duration < 1.0, f\"Average response time too high: {avg_duration:.3f}s\"\n        assert requests_per_second > 10, f\"Request rate too low: {requests_per_second:.2f} req/s\"\n    \n    def test_upload_endpoint_load(self, http_session, test_config, sample_image_file, services_ready):\n        \"\"\"Test upload endpoint under moderate load.\"\"\"\n        url = f\"{test_config['backend_url']}/api/v1/upload/image\"\n        \n        # Smaller load for upload tests (more resource intensive)\n        concurrent_requests = 5\n        total_requests = 10\n        \n        def upload_file() -> Dict[str, Any]:\n            start_time = time.time()\n            try:\n                with open(sample_image_file, 'rb') as f:\n                    files = {'file': (sample_image_file.name, f, 'image/jpeg')}\n                    response = http_session.post(url, files=files, timeout=60)  # Longer timeout for uploads\n                \n                duration = time.time() - start_time\n                \n                return {\n                    'success': response.status_code == 200,\n                    'status_code': response.status_code,\n                    'duration': duration,\n                    'error': None,\n                    'response_size': len(response.content)\n                }\n            except Exception as e:\n                duration = time.time() - start_time\n                return {\n                    'success': False,\n                    'status_code': 0,\n                    'duration': duration,\n                    'error': str(e),\n                    'response_size': 0\n                }\n        \n        # Execute upload load test\n        results = []\n        start_time = time.time()\n        \n        with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:\n            futures = [executor.submit(upload_file) for _ in range(total_requests)]\n            \n            for future in as_completed(futures):\n                result = future.result()\n                results.append(result)\n        \n        total_duration = time.time() - start_time\n        \n        # Analyze results\n        successful_uploads = [r for r in results if r['success']]\n        failed_uploads = [r for r in results if not r['success']]\n        \n        success_rate = len(successful_uploads) / len(results)\n        avg_duration = statistics.mean([r['duration'] for r in successful_uploads]) if successful_uploads else 0\n        \n        uploads_per_second = len(results) / total_duration\n        \n        # Print performance metrics\n        print(f\"\\nLoad Test Results (Upload Endpoint):\")\n        print(f\"Total uploads: {len(results)}\")\n        print(f\"Successful: {len(successful_uploads)} ({success_rate:.2%})\")\n        print(f\"Failed: {len(failed_uploads)}\")\n        print(f\"Uploads/second: {uploads_per_second:.2f}\")\n        print(f\"Average upload time: {avg_duration:.3f}s\")\n        \n        # Performance assertions (more lenient for uploads)\n        assert success_rate >= 0.8, f\"Success rate too low: {success_rate:.2%}\"\n        assert avg_duration < 30.0, f\"Average upload time too high: {avg_duration:.3f}s\"\n    \n    def test_mixed_load_scenario(self, http_session, test_config, sample_image_file, services_ready):\n        \"\"\"Test mixed load scenario with different endpoints.\"\"\"\n        \n        def health_check() -> Dict[str, Any]:\n            start_time = time.time()\n            try:\n                response = http_session.get(f\"{test_config['backend_url']}/health\", timeout=10)\n                return {\n                    'endpoint': 'health',\n                    'success': response.status_code == 200,\n                    'duration': time.time() - start_time\n                }\n            except Exception as e:\n                return {\n                    'endpoint': 'health',\n                    'success': False,\n                    'duration': time.time() - start_time,\n                    'error': str(e)\n                }\n        \n        def metrics_check() -> Dict[str, Any]:\n            start_time = time.time()\n            try:\n                response = http_session.get(f\"{test_config['backend_url']}/api/v1/health/metrics\", timeout=10)\n                return {\n                    'endpoint': 'metrics',\n                    'success': response.status_code == 200,\n                    'duration': time.time() - start_time\n                }\n            except Exception as e:\n                return {\n                    'endpoint': 'metrics',\n                    'success': False,\n                    'duration': time.time() - start_time,\n                    'error': str(e)\n                }\n        \n        def disk_usage_check() -> Dict[str, Any]:\n            start_time = time.time()\n            try:\n                response = http_session.get(f\"{test_config['backend_url']}/api/v1/admin/disk-usage\", timeout=10)\n                return {\n                    'endpoint': 'disk-usage',\n                    'success': response.status_code == 200,\n                    'duration': time.time() - start_time\n                }\n            except Exception as e:\n                return {\n                    'endpoint': 'disk-usage',\n                    'success': False,\n                    'duration': time.time() - start_time,\n                    'error': str(e)\n                }\n        \n        # Mix of different request types\n        tasks = [\n            health_check,\n            metrics_check,\n            disk_usage_check,\n        ] * 10  # 30 total requests, 10 of each type\n        \n        results = []\n        start_time = time.time()\n        \n        with ThreadPoolExecutor(max_workers=6) as executor:\n            futures = [executor.submit(task) for task in tasks]\n            \n            for future in as_completed(futures):\n                result = future.result()\n                results.append(result)\n        \n        total_duration = time.time() - start_time\n        \n        # Analyze results by endpoint\n        endpoint_stats = {}\n        for result in results:\n            endpoint = result['endpoint']\n            if endpoint not in endpoint_stats:\n                endpoint_stats[endpoint] = {'total': 0, 'successful': 0, 'durations': []}\n            \n            endpoint_stats[endpoint]['total'] += 1\n            if result['success']:\n                endpoint_stats[endpoint]['successful'] += 1\n                endpoint_stats[endpoint]['durations'].append(result['duration'])\n        \n        print(f\"\\nMixed Load Test Results:\")\n        print(f\"Total duration: {total_duration:.2f}s\")\n        print(f\"Overall requests/second: {len(results) / total_duration:.2f}\")\n        \n        for endpoint, stats in endpoint_stats.items():\n            success_rate = stats['successful'] / stats['total']\n            avg_duration = statistics.mean(stats['durations']) if stats['durations'] else 0\n            \n            print(f\"  {endpoint}: {stats['successful']}/{stats['total']} ({success_rate:.2%}) avg: {avg_duration:.3f}s\")\n            \n            # Each endpoint should have reasonable performance\n            assert success_rate >= 0.9, f\"{endpoint} success rate too low: {success_rate:.2%}\"\n    \n    def test_sustained_load(self, http_session, test_config, services_ready):\n        \"\"\"Test sustained load over a longer period.\"\"\"\n        url = f\"{test_config['backend_url']}/health\"\n        \n        duration_seconds = 60  # 1 minute sustained load\n        requests_per_second = 5\n        \n        results = []\n        start_time = time.time()\n        end_time = start_time + duration_seconds\n        \n        def make_request() -> Dict[str, Any]:\n            request_start = time.time()\n            try:\n                response = http_session.get(url, timeout=5)\n                return {\n                    'success': response.status_code == 200,\n                    'duration': time.time() - request_start,\n                    'timestamp': request_start\n                }\n            except Exception as e:\n                return {\n                    'success': False,\n                    'duration': time.time() - request_start,\n                    'timestamp': request_start,\n                    'error': str(e)\n                }\n        \n        # Sustained load with controlled rate\n        while time.time() < end_time:\n            batch_start = time.time()\n            \n            # Submit batch of requests\n            with ThreadPoolExecutor(max_workers=requests_per_second) as executor:\n                futures = [executor.submit(make_request) for _ in range(requests_per_second)]\n                \n                for future in as_completed(futures):\n                    result = future.result()\n                    results.append(result)\n            \n            # Wait until next second\n            elapsed = time.time() - batch_start\n            if elapsed < 1.0:\n                time.sleep(1.0 - elapsed)\n        \n        # Analyze sustained load results\n        successful_requests = [r for r in results if r['success']]\n        total_duration = time.time() - start_time\n        \n        success_rate = len(successful_requests) / len(results) if results else 0\n        actual_rps = len(results) / total_duration\n        avg_response_time = statistics.mean([r['duration'] for r in successful_requests]) if successful_requests else 0\n        \n        print(f\"\\nSustained Load Test Results:\")\n        print(f\"Duration: {total_duration:.1f}s\")\n        print(f\"Total requests: {len(results)}\")\n        print(f\"Success rate: {success_rate:.2%}\")\n        print(f\"Actual RPS: {actual_rps:.2f}\")\n        print(f\"Average response time: {avg_response_time:.3f}s\")\n        \n        # Performance assertions for sustained load\n        assert success_rate >= 0.95, f\"Success rate degraded under sustained load: {success_rate:.2%}\"\n        assert avg_response_time < 2.0, f\"Response time degraded under sustained load: {avg_response_time:.3f}s\"\n        assert actual_rps >= requests_per_second * 0.8, f\"Request rate too low: {actual_rps:.2f} vs target {requests_per_second}\"\n    \n    def test_memory_usage_stability(self, http_session, test_config, services_ready):\n        \"\"\"Test memory usage stability during load.\"\"\"\n        metrics_url = f\"{test_config['backend_url']}/api/v1/health/metrics\"\n        health_url = f\"{test_config['backend_url']}/health\"\n        \n        # Collect baseline memory\n        try:\n            response = http_session.get(metrics_url, timeout=10)\n            assert response.status_code == 200\n            \n            baseline_metrics = response.text\n            baseline_memory = self._extract_memory_metric(baseline_metrics)\n        except Exception:\n            pytest.skip(\"Memory metrics not available\")\n        \n        # Generate load for 30 seconds\n        load_duration = 30\n        start_time = time.time()\n        \n        request_count = 0\n        while time.time() - start_time < load_duration:\n            try:\n                http_session.get(health_url, timeout=5)\n                request_count += 1\n            except:\n                pass\n            time.sleep(0.1)  # 10 requests per second\n        \n        # Collect memory after load\n        time.sleep(5)  # Allow for any cleanup\n        \n        response = http_session.get(metrics_url, timeout=10)\n        assert response.status_code == 200\n        \n        final_metrics = response.text\n        final_memory = self._extract_memory_metric(final_metrics)\n        \n        if baseline_memory and final_memory:\n            memory_increase = final_memory - baseline_memory\n            memory_increase_percent = (memory_increase / baseline_memory) * 100\n            \n            print(f\"\\nMemory Stability Test:\")\n            print(f\"Requests made: {request_count}\")\n            print(f\"Baseline memory: {baseline_memory:.1f}%\")\n            print(f\"Final memory: {final_memory:.1f}%\")\n            print(f\"Memory increase: {memory_increase_percent:.1f}%\")\n            \n            # Memory increase should be reasonable\n            assert memory_increase_percent < 20, f\"Memory usage increased too much: {memory_increase_percent:.1f}%\"\n        else:\n            print(\"Memory metrics not available, skipping memory stability check\")\n    \n    def _extract_memory_metric(self, metrics_text: str) -> float:\n        \"\"\"Extract memory usage percentage from Prometheus metrics.\"\"\"\n        for line in metrics_text.split('\\n'):\n            if line.startswith('system_memory_usage_percent '):\n                try:\n                    return float(line.split(' ')[1])\n                except (IndexError, ValueError):\n                    continue\n        return None"